{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39be45cb-508d-4aa1-b6ac-c7d799298609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.distributed as dist\n",
    "\n",
    "import os, sys\n",
    "sys.path.insert(1, os.getcwd()) \n",
    "import random\n",
    "\n",
    "from diffusion_openai.video_datasets import load_data\n",
    "from diffusion_openai import dist_util, logger\n",
    "from diffusion_openai.script_util import (\n",
    "    NUM_CLASSES,\n",
    "    model_and_diffusion_defaults,\n",
    "    create_model_and_diffusion,\n",
    "    add_dict_to_argparser,\n",
    "    args_to_dict,\n",
    ")\n",
    "th.backends.cudnn.enabled = True  # Enable cuDNN\n",
    "th.backends.cudnn.benchmark = True  # Use cuDNN's auto-tuner for the best performance\n",
    "\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "416fe895-4d99-4091-a67f-f409d716c02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Parameters:\n",
    "    clip_denoised=True\n",
    "    num_samples=8\n",
    "    batch_size=8\n",
    "    use_ddim=False\n",
    "    model_path=\"\"\n",
    "    seq_len=20\n",
    "    sampling_type=\"generation\"\n",
    "    cond_frames=\"0,\"\n",
    "    cond_generation=True\n",
    "    resample_steps=1\n",
    "    data_dir=''\n",
    "    save_gt=False\n",
    "    seed=42\n",
    "    data_dir=\"/home/s_gladkykh/thesis/gif_dataset_64\"\n",
    "    batch_size=8\n",
    "    image_size=64\n",
    "    class_cond=False\n",
    "    deterministic=False\n",
    "    rgb=True\n",
    "    seq_len=20\n",
    "\n",
    "args = Parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be4405d1-1954-4442-b5fb-6e8f82190902",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = dict(\n",
    "    image_size=64,\n",
    "    class_cond=False,\n",
    "    learn_sigma=False,\n",
    "    sigma_small=False,\n",
    "    num_channels=128,\n",
    "    num_res_blocks=3,\n",
    "    scale_time_dim=0,\n",
    "    num_heads=4,\n",
    "    num_heads_upsample=1,\n",
    "    attention_resolutions=\"16,8\",\n",
    "    dropout=0.0,\n",
    "    diffusion_steps=1000,\n",
    "    noise_schedule=\"linear\",\n",
    "    timestep_respacing=\"\",\n",
    "    use_kl=False,\n",
    "    predict_xstart=False,\n",
    "    rescale_timesteps=True,\n",
    "    rescale_learned_sigmas=True,\n",
    "    use_checkpoint=False,\n",
    "    use_scale_shift_norm=True,\n",
    "    rgb=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0c39d9b-13a3-4c8e-8fd4-4a5ecc470c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Logging to /home/s_gladkykh/thesis/sky-diffusion/ramvid_notebooks/logs_sampling\n"
     ]
    }
   ],
   "source": [
    "dist_util.setup_dist()\n",
    "logger.configure(dir=\"/home/s_gladkykh/thesis/sky-diffusion/ramvid_notebooks/logs_sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf5c3f97-8aab-4495-bd05-d1940c3c7c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model and diffusion...\n"
     ]
    }
   ],
   "source": [
    "logger.log(\"creating model and diffusion...\")\n",
    "model, diffusion = create_model_and_diffusion(\n",
    "    **model_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4383e328-0090-43a4-bc94-c9c3bff8b598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(\n",
    "    dist_util.load_state_dict(\"/home/s_gladkykh/thesis/sky-diffusion/ramvid_notebooks/logs/model006000.pt\", map_location=\"cpu\")\n",
    ")\n",
    "model.to(dist_util.dev())\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1d1194e-412d-4f6b-8ed2-d2f850368035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cond_frames: [0]\n",
      "ref_frames: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "seq_len: 20\n"
     ]
    }
   ],
   "source": [
    "cond_kwargs = {}\n",
    "cond_frames = []\n",
    "if args.cond_generation:\n",
    "    data = load_data(\n",
    "        data_dir=\"/home/s_gladkykh/thesis/gif_dataset_64\",\n",
    "        batch_size=8,\n",
    "        image_size=64,\n",
    "        class_cond=False,\n",
    "        deterministic=False,\n",
    "        rgb=True,\n",
    "        seq_len=20\n",
    "    )\n",
    "    \n",
    "    num = \"\"\n",
    "    for i in args.cond_frames:\n",
    "        if i == \",\":\n",
    "            cond_frames.append(int(num))\n",
    "            num = \"\"\n",
    "        else:\n",
    "            num = num + i\n",
    "    print(num)\n",
    "    ref_frames = list(i for i in range(args.seq_len) if i not in cond_frames)\n",
    "    logger.log(f\"cond_frames: {cond_frames}\")\n",
    "    logger.log(f\"ref_frames: {ref_frames}\")\n",
    "    logger.log(f\"seq_len: {args.seq_len}\")\n",
    "    cond_kwargs[\"resampling_steps\"] = args.resample_steps\n",
    "cond_kwargs[\"cond_frames\"] = cond_frames\n",
    "cond_kwargs[\"saver\"] = None\n",
    "\n",
    "if args.rgb:\n",
    "    channels = 3\n",
    "else:\n",
    "    channels = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "407ee2fe-0216-4a86-afbc-86acdfdb33f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling...\n",
      "created 8 samples\n"
     ]
    }
   ],
   "source": [
    "logger.log(\"sampling...\")\n",
    "all_videos = []\n",
    "all_gt = []\n",
    "while len(all_videos) * args.batch_size < args.num_samples:\n",
    "    \n",
    "    if args.cond_generation:\n",
    "        video, _ = next(data)\n",
    "        cond_kwargs[\"cond_img\"] = video[:,:,cond_frames].to(dist_util.dev()) \n",
    "        video = video.to(dist_util.dev())\n",
    "\n",
    "\n",
    "    sample_fn = (\n",
    "        diffusion.p_sample_loop if not args.use_ddim else diffusion.ddim_sample_loop\n",
    "    )\n",
    "\n",
    "    sample = sample_fn(\n",
    "        model,\n",
    "        (args.batch_size, channels, args.seq_len, args.image_size, args.image_size),\n",
    "        clip_denoised=args.clip_denoised,\n",
    "        progress=False,\n",
    "        cond_kwargs=cond_kwargs\n",
    "    )\n",
    "\n",
    "    sample = ((sample + 1) * 127.5).clamp(0, 255).to(th.uint8)\n",
    "    sample = sample.permute(0, 2, 3, 4, 1)\n",
    "    sample = sample.contiguous()\n",
    "\n",
    "    gathered_samples = [th.zeros_like(sample) for _ in range(dist.get_world_size())]\n",
    "    dist.all_gather(gathered_samples, sample)  # gather not supported with NCCL\n",
    "    all_videos.extend([sample.cpu().numpy() for sample in gathered_samples])\n",
    "    logger.log(f\"created {len(all_videos) * args.batch_size} samples\")\n",
    "\n",
    "    if args.cond_generation and args.save_gt:\n",
    "\n",
    "        video = ((video + 1) * 127.5).clamp(0, 255).to(th.uint8)\n",
    "        video = video.permute(0, 2, 3, 4, 1)\n",
    "        video = video.contiguous()\n",
    "\n",
    "        gathered_videos = [th.zeros_like(video) for _ in range(dist.get_world_size())]\n",
    "        dist.all_gather(gathered_videos, video)  # gather not supported with NCCL\n",
    "        all_gt.extend([video.cpu().numpy() for video in gathered_videos])\n",
    "        logger.log(f\"created {len(all_gt) * args.batch_size} videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0705471-69dc-43a0-9cea-a8ae3c42da93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ace78fa9-6cb0-4caf-9152-0660e77fb933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving samples to /home/s_gladkykh/thesis/sky-diffusion/ramvid_notebooks/logs_sampling/8x20x64x64x3\n",
      "sampling complete\n"
     ]
    }
   ],
   "source": [
    "arr = np.concatenate(all_videos, axis=0)\n",
    "\n",
    "if args.cond_generation and args.save_gt:\n",
    "    arr_gt = np.concatenate(all_gt, axis=0)\n",
    "\n",
    "\n",
    "if dist.get_rank() == 0:\n",
    "\n",
    "    shape_str = \"x\".join([str(x) for x in arr.shape])\n",
    "    logger.log(f\"saving samples to {os.path.join(logger.get_dir(), shape_str)}\")\n",
    "    np.savez(os.path.join(logger.get_dir(), shape_str), arr)\n",
    "\n",
    "    if args.cond_generation and args.save_gt:\n",
    "        shape_str_gt = \"x\".join([str(x) for x in arr_gt.shape])\n",
    "        logger.log(f\"saving ground_truth to {os.path.join(logger.get_dir(), shape_str_gt)}\")\n",
    "        np.savez(os.path.join(logger.get_dir(), shape_str_gt), arr_gt)\n",
    "\n",
    "dist.barrier()\n",
    "logger.log(\"sampling complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6368b511-3bdc-4a19-bf0a-f045483e0c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 20, 64, 64, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_videos[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8b7f06b-c9ac-407e-8053-3e7840e10695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imageio\n",
    "\n",
    "all_videos = all_videos[0]\n",
    "for i in range(all_videos.shape[0]):\n",
    "    # Create a writer object for GIF\n",
    "    writer = imageio.get_writer(f'samples/video_{i}.gif', mode='I', duration=0.1)  # Adjust duration as needed\n",
    "    \n",
    "    # Iterate over each frame in the sequence\n",
    "    for j in range(all_videos.shape[1]):\n",
    "        # Append each frame to the writer object\n",
    "        writer.append_data(all_videos[i, j])\n",
    "    \n",
    "    # Close the writer object\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86761c2-fa35-4e57-8af1-6aaa310b2908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
