{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c0f23a1-f0e1-42b3-866e-67654ce61a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cond_unet import UNet\n",
    "from cond_ddim import DDIMScheduler\n",
    "from cond_sky_dataset import SkyDataset\n",
    "from ema import EMA\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from diffusers.optimization import get_scheduler\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm.auto import tqdm\n",
    "from utils import save_images, normalize_to_neg_one_to_one, unnormalize_to_zero_to_one\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cfdbcdb-7481-4fdf-b859-53e0815ea29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    resolution = 128\n",
    "    n_timesteps = 1000\n",
    "    learning_rate = 1e-3\n",
    "    adam_beta1 = 0.9\n",
    "    adam_beta2 = 0.99\n",
    "    adam_weight_decay = 0.0\n",
    "    train_batch_size = 24\n",
    "    eval_batch_size = 24\n",
    "    num_epochs = 200\n",
    "    gradient_accumulation_steps = 1\n",
    "    gamma = 0.996\n",
    "    lr_scheduler = \"cosine\"\n",
    "    lr_warmup_steps = 100\n",
    "    fp16_precision = True\n",
    "    use_clip_grad = False\n",
    "    save_model_steps = 1000\n",
    "    samples_dir = \"cond_samples\"\n",
    "    dataset_name = \"SkyDiffusion_1e-3_50\"\n",
    "    n_inference_timesteps = 250\n",
    "    output_dir = \"cond_models/SkyDiffusion1e-3_50.pth\"\n",
    "\n",
    "args = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fba8b93-99aa-470e-a8be-276236abeb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d35bfed9-25da-4772-9c46-74ee89afc7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(3, image_size=args.resolution, hidden_dims=[64, 128, 256, 512])\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "654a5684-556c-43f9-aaf1-5996b3ad7233",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler = DDIMScheduler(num_train_timesteps=args.n_timesteps,\n",
    "                                beta_schedule=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ace5fc3e-bd74-46b7-8caf-8eec8ec438d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=args.learning_rate,\n",
    "    betas=(args.adam_beta1, args.adam_beta2),\n",
    "    weight_decay=args.adam_weight_decay,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e24697a4-4153-4a9b-9662-5b19288aac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = transforms.Compose([\n",
    "    transforms.Resize((args.resolution, args.resolution)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c388da6d-c455-4da6-81b2-ee8cb7d3bd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SkyDataset(transform=tfms)\n",
    "train_dataloader = DataLoader(dataset=dataset, batch_size=args.train_batch_size, shuffle=True)\n",
    "steps_per_epcoch = len(train_dataloader)\n",
    "total_num_steps = (steps_per_epcoch * args.num_epochs) // args.gradient_accumulation_steps\n",
    "total_num_steps += int(total_num_steps * 10/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fbf2b4c-7a9f-403f-9864-89b5ad8737b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_sample = next(iter(train_dataloader))\n",
    "global_conditioning = some_sample[0].to(device)\n",
    "save_images({\"sample_pt\": global_conditioning}, \"real\", args)\n",
    "prediction = some_sample[1].to(device)\n",
    "save_images({\"sample_pt\": prediction}, \"prediction\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17a3ce5a-b1ba-4ae0-b785-c0d9317a24f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = args.gamma\n",
    "ema = EMA(model, gamma, total_num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3932e868-9893-4b14-8877-4dceedbae1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = get_scheduler(\n",
    "    args.lr_scheduler,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.lr_warmup_steps,\n",
    "    num_training_steps=total_num_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "830ce407-68d6-4c7f-92fd-d4b95a6fb4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = GradScaler(enabled=args.fp16_precision)\n",
    "global_step = 0\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aca140-9f94-4c3b-abac-6ce670c629b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e818eebdc0fb46c4999449b9323e1fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/769 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [01:01<00:00,  4.08it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23385f90ffc941adb73bc0de6ba3c748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/769 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [01:01<00:00,  4.08it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b711e9bd3f44c6892482b7499716d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/769 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(args.num_epochs):\n",
    "    progress_bar = tqdm(total=steps_per_epcoch)\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    losses_log = 0\n",
    "    for step, (cond_batch, batch) in enumerate(train_dataloader):        \n",
    "        clean_images = batch.to(device)\n",
    "        clean_images = normalize_to_neg_one_to_one(clean_images)\n",
    "\n",
    "        cond_clean_images = cond_batch.to(device)\n",
    "        cond_clean_images = normalize_to_neg_one_to_one(cond_clean_images)\n",
    "\n",
    "        batch_size = clean_images.shape[0]\n",
    "        noise = torch.randn(clean_images.shape).to(device)\n",
    "\n",
    "        timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (batch_size,), device=device).long()\n",
    "        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(enabled=args.fp16_precision):\n",
    "            noise_pred = model(noisy_images, cond_clean_images, timesteps)\n",
    "            loss = F.l1_loss(noise_pred, noise)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        ema.update_params(gamma)\n",
    "        gamma = ema.update_gamma(global_step)\n",
    "\n",
    "        if args.use_clip_grad:\n",
    "            clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        progress_bar.update(1)\n",
    "        losses_log += loss.detach().item()\n",
    "        logs = {\n",
    "            \"loss_avg\": losses_log / (step + 1),\n",
    "            \"loss\": loss.detach().item(),\n",
    "            \"lr\": lr_scheduler.get_last_lr()[0],\n",
    "            \"step\": global_step,\n",
    "            \"gamma\": gamma\n",
    "        }\n",
    "\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        global_step += 1\n",
    "\n",
    "    progress_bar.close()\n",
    "    losses.append(losses_log / (step + 1)) \n",
    "\n",
    "    ema.ema_model.eval()\n",
    "    with torch.no_grad():\n",
    "        # has to be instantiated every time, because of reproducibility\n",
    "        generator = torch.manual_seed(0)\n",
    "        generated_images = noise_scheduler.generate(\n",
    "            ema.ema_model,\n",
    "            conditioning=global_conditioning.to(device),\n",
    "            num_inference_steps=args.n_inference_timesteps,\n",
    "            generator=generator,\n",
    "            eta=1.0,\n",
    "            use_clipped_model_output=True,\n",
    "            batch_size=args.eval_batch_size,\n",
    "            output_type=\"numpy\")\n",
    "\n",
    "        save_images(generated_images, epoch, args)\n",
    "\n",
    "        torch.save(\n",
    "            {\n",
    "                'model_state': model.state_dict(),\n",
    "                'ema_model_state': ema.ema_model.state_dict(),\n",
    "                'optimizer_state': optimizer.state_dict(),\n",
    "            }, args.output_dir\n",
    "        )\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
