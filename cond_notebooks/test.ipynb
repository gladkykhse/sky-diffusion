{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17c24fc9-6b47-461f-8231-15fa4c8d2e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet import UNet\n",
    "from ddim import DDIMScheduler\n",
    "from sky_dataset import SkyDataset\n",
    "from ema import EMA\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from diffusers.optimization import get_scheduler\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm.auto import tqdm\n",
    "from utils import save_images, normalize_to_neg_one_to_one\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3c4fe8d-d61a-45e1-96ef-075e5cf3ce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    resolution = 32\n",
    "    n_timesteps = 1000\n",
    "    learning_rate = 5e-6\n",
    "    adam_beta1 = 0.9\n",
    "    adam_beta2 = 0.99\n",
    "    adam_weight_decay = 0.0\n",
    "    train_batch_size = 16\n",
    "    eval_batch_size = 16\n",
    "    num_epochs = 50\n",
    "    gradient_accumulation_steps = 1\n",
    "    gamma = 0.996\n",
    "    lr_scheduler = \"cosine\"\n",
    "    lr_warmup_steps = 100\n",
    "    fp16_precision = True\n",
    "    use_clip_grad = False\n",
    "    save_model_steps = 1000\n",
    "    samples_dir = \"samples\"\n",
    "    dataset_name = \"SkyDiffusion3\"\n",
    "    n_inference_timesteps = 100\n",
    "    output_dir = \"models/SkyDiffusion3.pth\"\n",
    "    \n",
    "\n",
    "args = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11e5159b-1653-4bfd-a4c4-f6504ddc4c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72afed1a-d551-4f84-ae5d-8c5848e753f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(3, image_size=args.resolution, hidden_dims=[64, 128, 256, 512])\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbcd118f-1f2e-45b5-8617-8c77fc2287c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler = DDIMScheduler(num_train_timesteps=args.n_timesteps,\n",
    "                                beta_schedule=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "099938b6-ac5f-48a0-973b-d2ae98f74273",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=args.learning_rate,\n",
    "    betas=(args.adam_beta1, args.adam_beta2),\n",
    "    weight_decay=args.adam_weight_decay,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28f2e281-d87c-439f-b306-f362e540bc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = transforms.Compose([\n",
    "    transforms.Resize((args.resolution, args.resolution)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e7bc00f-55a9-4c83-8de1-bbf917e229a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SkyDataset(transform=tfms)\n",
    "train_dataloader = DataLoader(dataset=dataset, batch_size=args.train_batch_size, shuffle=True)\n",
    "steps_per_epcoch = len(train_dataloader)\n",
    "total_num_steps = (steps_per_epcoch * args.num_epochs) // args.gradient_accumulation_steps\n",
    "total_num_steps += int(total_num_steps * 10/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52a891e7-8257-4d63-843e-875da33ae5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = args.gamma\n",
    "ema = EMA(model, gamma, total_num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "662f694f-ad72-4fe1-8914-3651a3dcc6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = get_scheduler(\n",
    "    args.lr_scheduler,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.lr_warmup_steps,\n",
    "    num_training_steps=total_num_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7f4d478-e4be-4a1c-b2b3-10a1b13796fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = GradScaler(enabled=args.fp16_precision)\n",
    "global_step = 0\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a675b96-44b6-4674-984b-45bb456cd38f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db5ce348534241de8065aaccb1c0a0d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1178 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 61.66it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "425ffcc1da764ad1858405fe8f76caa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1178 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(args.num_epochs):\n",
    "    progress_bar = tqdm(total=steps_per_epcoch)\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    losses_log = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        clean_images = batch.to(device)\n",
    "        clean_images = normalize_to_neg_one_to_one(clean_images)\n",
    "\n",
    "        batch_size = clean_images.shape[0]\n",
    "        noise = torch.randn(clean_images.shape).to(device)\n",
    "\n",
    "        timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (batch_size,), device=device).long()\n",
    "        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(enabled=args.fp16_precision):\n",
    "            noise_pred = model(noisy_images, timesteps)\n",
    "            loss = F.l1_loss(noise_pred, noise)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        ema.update_params(gamma)\n",
    "        gamma = ema.update_gamma(global_step)\n",
    "\n",
    "        if args.use_clip_grad:\n",
    "            clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        progress_bar.update(1)\n",
    "        losses_log += loss.detach().item()\n",
    "        logs = {\n",
    "            \"loss_avg\": losses_log / (step + 1),\n",
    "            \"loss\": loss.detach().item(),\n",
    "            \"lr\": lr_scheduler.get_last_lr()[0],\n",
    "            \"step\": global_step,\n",
    "            \"gamma\": gamma\n",
    "        }\n",
    "\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        global_step += 1\n",
    "\n",
    "    progress_bar.close()\n",
    "    losses.append(losses_log / (step + 1)) \n",
    "\n",
    "    ema.ema_model.eval()\n",
    "    with torch.no_grad():\n",
    "        # has to be instantiated every time, because of reproducibility\n",
    "        generator = torch.manual_seed(0)\n",
    "        generated_images = noise_scheduler.generate(\n",
    "            ema.ema_model,\n",
    "            num_inference_steps=args.n_inference_timesteps,\n",
    "            generator=generator,\n",
    "            eta=1.0,\n",
    "            use_clipped_model_output=True,\n",
    "            batch_size=args.eval_batch_size,\n",
    "            output_type=\"numpy\")\n",
    "\n",
    "        save_images(generated_images, epoch, args)\n",
    "\n",
    "        torch.save(\n",
    "            {\n",
    "                'model_state': model.state_dict(),\n",
    "                'ema_model_state': ema.ema_model.state_dict(),\n",
    "                'optimizer_state': optimizer.state_dict(),\n",
    "            }, args.output_dir\n",
    "        )\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
